{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Training - Spark DF vs Pandas DF, GLM implementation using SparkML, Pros and Cons of SparkML\n",
    "\n",
    "**Objective 1:** Understand, document and provide example on data input-ouput in Spark. Differences between Spark DFs and Pandas DFs?\n",
    "\n",
    "**Objective 2:** Build GLM models using SparkML library (replicate analysis on AutoClaims data)\n",
    "\n",
    "**Objective 3:** Understand and demonstrate Pro's and Con's of using SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Spark?**\n",
    "\n",
    "- A big data processing framework written in scala \n",
    "- provides other languages like Java, Python and R\n",
    "- PySpark - Python API for Spark\n",
    "- efficiently working with huge amounts of data, which do not fit into the memory of a single computer\n",
    "- distributed processing\n",
    "- Initially Spark only provided RDDs API but later it picked up the good ideas of data frames in Pandas and R, which is now the preferred API to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Additional Info:***\n",
    "- RDDs - resilient distributed datasets - one dimensional data - performing transformation on unstructured data at a lower level than DF - ex: media streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a spark session** \n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/ <br>\n",
    "https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html?highlight=sparksession\n",
    "\n",
    "- entry point to Spark introduced with Spark 2.0 \n",
    "- SparkSession is a combined class for all different contexts we used to have prior to 2.0 relase - SparkContext, SQL Context, HIVE Context\n",
    "- Prior Spark 2.0, Spark Context was the entry point of any spark application and for any other spark interactions we had to create separate context - SQL, HIVE, etc\n",
    "- you can only create one spark session per application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Additional Info:***\n",
    "- sparkConf is required to create the spark context object, which stores configuration parameter like appName (to identify your spark driver), application, number of core and memory size of executor running on worker node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://UKDVDV525.bfl.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b883f47d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Training\").getOrCreate()     \n",
    "# “spark://master:7077” to run on a Spark cluster or local[4] to run locally with 4 cores\n",
    "# the name of your app\n",
    "#.config(\"spark.some.config.option\", \"some-value\")    # config how much memory to use, number of cores, etc\n",
    "#.enableHiveSupport()                                 # Gets an existing SparkSession or creates a new one\n",
    "\n",
    "                                            \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 1\n",
    "Understand, document and provide example on data input-ouput in Spark (RDD). How does this fit with current Python / Pandas type workflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark data frame supports reading data from formats like json, parquet, HIVE table, existing RDD – be it from local file system, distributed file system (HDFS), cloud storage, external databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDD example <br> \n",
    "https://sparkbyexamples.com/apache-spark-rdd/different-ways-to-create-spark-rdd/#from-parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ankit', 25), ('Jalfaizy', 22), ('saurabh', 20), ('Bala', 26)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "# a list of tuples\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "sc = spark.sparkContext\n",
    "# partition the data and distribute it to rdds\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Loading csv as a spark df using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "|      ID|KIDSDRIV|  BIRTH|AGE|HOMEKIDS|YOJ| INCOME|PARENT1|HOME_VAL|MSTATUS|GENDER|EDUCATION|  OCCUPATION|TRAVTIME|CAR_USE|BLUEBOOK|TIF|CAR_TYPE|RED_CAR|OLDCLAIM|CLM_FREQ|REVOKED|MVR_PTS|CLM_AMT|CAR_AGE|CLAIM_FLAG|         URBANICITY|\n",
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "|63581743|       0|16MAR39| 60|       0| 11|$67,349|     No|      $0|   z_No|     M|      PhD|Professional|      14|Private| $14,230| 11| Minivan|    yes|  $4,461|       2|     No|      3|     $0|     18|         0|Highly Urban/ Urban|\n",
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark has an integrated function to read csv\n",
    "# if we don't do inferSchema everything will be a string\n",
    "spark_df = spark.read.csv(\"car_insurance_claim.csv\", header='true', inferSchema=True)\n",
    "#print it\n",
    "spark_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- KIDSDRIV: integer (nullable = true)\n",
      " |-- BIRTH: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- HOMEKIDS: integer (nullable = true)\n",
      " |-- YOJ: integer (nullable = true)\n",
      " |-- INCOME: string (nullable = true)\n",
      " |-- PARENT1: string (nullable = true)\n",
      " |-- HOME_VAL: string (nullable = true)\n",
      " |-- MSTATUS: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- OCCUPATION: string (nullable = true)\n",
      " |-- TRAVTIME: integer (nullable = true)\n",
      " |-- CAR_USE: string (nullable = true)\n",
      " |-- BLUEBOOK: string (nullable = true)\n",
      " |-- TIF: integer (nullable = true)\n",
      " |-- CAR_TYPE: string (nullable = true)\n",
      " |-- RED_CAR: string (nullable = true)\n",
      " |-- OLDCLAIM: string (nullable = true)\n",
      " |-- CLM_FREQ: integer (nullable = true)\n",
      " |-- REVOKED: string (nullable = true)\n",
      " |-- MVR_PTS: integer (nullable = true)\n",
      " |-- CLM_AMT: string (nullable = true)\n",
      " |-- CAR_AGE: integer (nullable = true)\n",
      " |-- CLAIM_FLAG: integer (nullable = true)\n",
      " |-- URBANICITY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# as in columns and dtypes in pandas\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Loading csv as a spark df using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df2 = spark.createDataFrame(pd.read_csv(\"car_insurance_claim.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exporting Data**\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-read-write-dataframe-parquet-example/ <br>\n",
    "https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4 <br>\n",
    "https://towardsdatascience.com/a-brief-introduction-to-pyspark-ff4284701873 <br>\n",
    "https://mungingdata.com/apache-spark/output-one-file-csv-parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. If we are going to process results with Spark then parquet is a good format to use for saving data frames.\n",
    "- column names and data types are preserved\n",
    "- When saving a dataframe in parquet format, it is often partitioned into multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode - append or overwrite\n",
    "# partition by - creates a folder hierarchy for each spark partition - gender folder and then salary within the gender folder\n",
    "\n",
    "spark_df.write.mode('overwrite').partitionBy(\"gender\",\"salary\").parquet(\"/tmp/output/claims_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/partitions.PNG\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- manual partitioning of files\n",
    "- 2 ways to do it: repartition and coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of partitions\n",
    "spark_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark splits data into partitions and executes computations on the partitions in parallel\n",
    "\n",
    "# coalesce\n",
    "# imagine we have A,B,C,D - coalesce is reducing data moving, B moved to A and D moved to C\n",
    "# you can't increase the number of partitions to > 4\n",
    "spark_df.coalesce(2).write.mode().parquet()\n",
    "\n",
    "# repartition\n",
    "# you can increase and decrease\n",
    "# shuffles the data and then do partitioning without trying to minimize the data movement\n",
    "spark_df.repartition(5).write.mode().parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Writing out a csv file\n",
    "- all of the data will be pulled to a single node before being output to CSV\n",
    "- recommended when you need to save a small dataframe and process it in a system outside of Spark\n",
    "- we can control the directory but not the file name when saving locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition can be used too\n",
    "spark_df.coalesce(1).write.csv(\"home/Documents/tmp/one-file-coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/one_file_csv.PNG\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **When is Spark useful**\n",
    "\n",
    "https://towardsdatascience.com/spark-vs-pandas-part-2-spark-c57f8ea3a781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **How a spark df is different from a pandas df**\n",
    "\n",
    "**1. Schema**\n",
    "- Spark df is built on the concept of columns and rows with the set of columns implicitly implying a schema shared among all rows\n",
    "- In contrast to Pandas, the schema also dictates the data type for each column that can be stored in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b\n",
       "0  1    3\n",
       "1  1  NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x = pd.DataFrame(columns=['a','b'],data={'a':[1,'1'],'b':['3',np.nan]})\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "|      ID|KIDSDRIV|  BIRTH|AGE|HOMEKIDS|YOJ| INCOME|PARENT1|HOME_VAL|MSTATUS|GENDER|EDUCATION|  OCCUPATION|TRAVTIME|CAR_USE|BLUEBOOK|TIF|CAR_TYPE|RED_CAR|OLDCLAIM|CLM_FREQ|REVOKED|MVR_PTS|CLM_AMT|CAR_AGE|CLAIM_FLAG|         URBANICITY|\n",
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "|63581743|       0|16MAR39| 60|       0| 11|$67,349|     No|      $0|   z_No|     M|      PhD|Professional|      14|Private| $14,230| 11| Minivan|    yes|  $4,461|       2|     No|      3|     $0|     18|         0|Highly Urban/ Urban|\n",
      "+--------+--------+-------+---+--------+---+-------+-------+--------+-------+------+---------+------------+--------+-------+--------+---+--------+-------+--------+--------+-------+-------+-------+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Index**\n",
    "- as opposed to pd, Spark doesn’t support any indexing for efficient access to individual rows in a DataFrame\n",
    "    - since all transformations are always performed on all records, Spark will reorganize the data as needed on the fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Data Scaling**\n",
    "\n",
    "- spark is designed to scale with data in terms of number of rows not number of columns\n",
    "    - it can easily work with billions of rows but the number of columns should be limited (hundreds to a couple of thousands)\n",
    "    - you cannot simply perform a transpose operation in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Nested data structures**\n",
    "- as opposed to pandas, spark does not support indices nor nested indices but it offers very good support for deeply nested data structures like in json docs\n",
    "    - these are often used in the internal communication between apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/json_nested_data.PNG\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tabular representation below doesn’t really show the complex nature of the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/twitter_nested.PNG\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- however, the schema does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tweets_schema.PNG\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Execution**\n",
    "- spark uses a lazy execution model\n",
    "- transformations on a data set are not processed immediately \n",
    "- Spark will start to work when the result of all transformations is required\n",
    "    - displaying some records on the console \n",
    "    - when you write the results into a file\n",
    "- Spark has the ability to optimize the whole plan before execution instead of blindingly following the steps as specified by the developer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Processing scalability**\n",
    "- breaks down work into individual tasks to be processed in parallel and make use of the resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Data Scalability**\n",
    "- scales very well with huge amounts of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Transformations**\n",
    "##### A broad set of transformations - uses SQL wording but not the syntax in all its methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- projection - creating a new data set with less columns - similar to select in SQL <br>\n",
    "##### Ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spark_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the variables for the model\n",
    "important_vars = ['AGE','GENDER','CAR_USE','BLUEBOOK','CAR_TYPE','URBANICITY','MSTATUS']\n",
    "spark_df = spark_df.select(important_vars+ ['CLAIM_FLAG','CLM_AMT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filtering - selecting subsets of rows - where in SQL <br>\n",
    "##### Ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6513"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F \n",
    "\n",
    "private = spark_df.where(F.col('CAR_USE') == 'Private')\n",
    "private.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(AGE=60, GENDER='M', CAR_USE='Private', BLUEBOOK='$14,230', CAR_TYPE='Minivan', URBANICITY='Highly Urban/ Urban', MSTATUS='z_No', CLAIM_FLAG=0, CLM_AMT='$0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concatenation - it is only vertical\n",
    "    - adding rows from a second DataFrame with the same columns\n",
    "- join is to do horizontal concatenation - records with similar join keys are sent to the same machine and then the join is executed in parallel\n",
    "    - tricky if no natural join key is available \n",
    "\n",
    "##### Ex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b  c    d\n",
       "0  1.0  3.0  6  NaN\n",
       "1  2.0  4.0  7  NaN\n",
       "0  NaN  NaN  5  8.0\n",
       "1  NaN  NaN  6  9.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x = pd.DataFrame(columns=('a','b','c'),data={'a':[1,2],'b':[3,4],'c':[6,7]})\n",
    "y = pd.DataFrame(columns=('c','d'),data={'c':[5,6],'d':[8,9]})\n",
    "\n",
    "# vertical concatenation\n",
    "pd.concat([x,y],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c  c  d\n",
       "0  1  3  6  5  8\n",
       "1  2  4  7  6  9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# horizontal concatenation - adding columns from two data frames\n",
    "# this is not possible in spark unless we join the tables\n",
    "pd.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10302, 6513, 3789)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_private = spark_df.where(F.col('CAR_USE') != 'Private')\n",
    "\n",
    "spark_df_concat = private.union(non_private)\n",
    "spark_df_concat.count(), private.count(), non_private.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aggregations \n",
    "    - similar wording as in SQL - min, max, sum, etc\n",
    "    - column wise aggregation is possible in spark as in pandas as shown below\n",
    "        - as said above spark is developed to scale with the number of rows not columns\n",
    "    - row-wise aggregations over columns is to be done manually as opposed to pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------------+\n",
      "|min(AGE)|max(AGE)|         avg(AGE)|\n",
      "+--------+--------+-----------------+\n",
      "|      16|      81|45.01629014907023|\n",
      "+--------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column wise aggregation is possible in spark as in pandas\n",
    "age_agg = private.select(F.min('AGE'), F.max('AGE'), F.avg('AGE'))\n",
    "age_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(AGE_2nd_driver=55, AGE=60)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private = private.withColumn('AGE_2nd_driver', F.col('AGE')-F.lit(5))\n",
    "private.select(['AGE_2nd_driver','AGE']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(AGE_2nd_driver=55, AGE=60, AVG_AGE=57.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg age\n",
    "private = private.withColumn('AVG_AGE',(F.col('AGE')+F.col('AGE_2nd_driver'))/2)\n",
    "private.select(['AGE_2nd_driver','AGE','AVG_AGE']).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 2:\n",
    "Build GLM models using SparkML library (replicate analysis on AutoClaims data) <br>\n",
    "\n",
    "https://people.stat.sc.edu/haigang/GLM_in_spark.html <br>\n",
    "https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a <br>\n",
    "https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa <br>\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html <br>\n",
    "https://goodboychan.github.io/chans_jupyter/python/datacamp/pyspark/machine_learning/2020/08/10/01-Model-tuning-and-selection-in-PySpark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=63581743, KIDSDRIV=0, BIRTH='16MAR39', AGE=60, HOMEKIDS=0, YOJ=11, INCOME=67349.0, PARENT1='No', HOME_VAL=0.0, MSTATUS='z_No', GENDER='M', EDUCATION='PhD', OCCUPATION='Professional', TRAVTIME=14, CAR_USE='Private', BLUEBOOK=14230.0, TIF=11, CAR_TYPE='Minivan', RED_CAR='yes', OLDCLAIM=4461.0, CLM_FREQ=2, REVOKED='No', MVR_PTS=3, CLM_AMT=0.0, CAR_AGE=18, CLAIM_FLAG=0, URBANICITY='Highly Urban/ Urban')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert object to numerical\n",
    "for col in ['INCOME','HOME_VAL','BLUEBOOK','OLDCLAIM', 'CLM_AMT']:\n",
    "    spark_df = spark_df.withColumn(col, F.regexp_replace(spark_df[col], r\"([^.0-9])\", \"\"))\n",
    "    spark_df = spark_df.withColumn(col, spark_df[col].cast(\"float\"))\n",
    "    #spark_df = spark_df.fillna({col:0})\n",
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ID=63581743, KIDSDRIV=0, BIRTH='16MAR39', AGE=60, HOMEKIDS=0, YOJ=11, INCOME=67349.0, PARENT1='No', HOME_VAL=0.0, MSTATUS='NO', GENDER='M', EDUCATION='PhD', OCCUPATION='Professional', TRAVTIME=14, CAR_USE='Private', BLUEBOOK=14230.0, TIF=11, CAR_TYPE='Minivan', RED_CAR='yes', OLDCLAIM=4461.0, CLM_FREQ=2, REVOKED='No', MVR_PTS=3, CLM_AMT=0.0, CAR_AGE=18, CLAIM_FLAG=0, URBANICITY='Highly Urban/ Urban')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean text classes\n",
    "spark_df = spark_df.withColumn('MSTATUS', F.upper(F.col('MSTATUS')))\n",
    "spark_df = spark_df.withColumn('MSTATUS', F.regexp_replace(spark_df['MSTATUS'], r\"([Z_])\", \"\"))\n",
    "spark_df = spark_df.withColumn('MSTATUS', F.regexp_replace(spark_df['MSTATUS'], r\"([^A-Z])\", \"\"))\n",
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# encodes a string column of labels to a column of label indices\n",
    "stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in ['GENDER','CAR_USE','CAR_TYPE','URBANICITY','MSTATUS']]\n",
    "stage_one_hot = [OneHotEncoder(inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in ['GENDER','CAR_USE','CAR_TYPE','URBANICITY','MSTATUS']]\n",
    "\n",
    "ppl = Pipeline(stages = stage_string + stage_one_hot)\n",
    "spark_df = ppl.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(AGE=60, GENDER='M', CAR_USE='Private', BLUEBOOK=14230.0, CAR_TYPE='Minivan', URBANICITY='Highly Urban/ Urban', MSTATUS='NO', CLAIM_FLAG=0, CLM_AMT=0.0, GENDER_string_encoded=1.0, CAR_USE_string_encoded=0.0, CAR_TYPE_string_encoded=1.0, URBANICITY_string_encoded=0.0, MSTATUS_string_encoded=1.0, GENDER_one_hot=SparseVector(1, {}), CAR_USE_one_hot=SparseVector(1, {0: 1.0}), CAR_TYPE_one_hot=SparseVector(5, {1: 1.0}), URBANICITY_one_hot=SparseVector(1, {0: 1.0}), MSTATUS_one_hot=SparseVector(1, {}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer, VectorAssembler, StandardScaler\n",
    "\n",
    "# merges columns into a vector column\n",
    "vector_assembler = VectorAssembler(inputCols = ['AGE','BLUEBOOK'], outputCol = \"numericals\")\n",
    "spark_df = vector_assembler.transform(spark_df)\n",
    "\n",
    "scaler = StandardScaler(inputCol = \"numericals\", outputCol = \"numericals_after_scale\")\n",
    "\n",
    "ppl2 = Pipeline(stages = [scaler])\n",
    "spark_df = ppl2.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(AGE=60, GENDER='M', CAR_USE='Private', BLUEBOOK=14230.0, CAR_TYPE='Minivan', URBANICITY='Highly Urban/ Urban', MSTATUS='NO', CLAIM_FLAG=0, CLM_AMT=0.0, GENDER_string_encoded=1.0, CAR_USE_string_encoded=0.0, CAR_TYPE_string_encoded=1.0, URBANICITY_string_encoded=0.0, MSTATUS_string_encoded=1.0, GENDER_one_hot=SparseVector(1, {}), CAR_USE_one_hot=SparseVector(1, {0: 1.0}), CAR_TYPE_one_hot=SparseVector(5, {1: 1.0}), URBANICITY_one_hot=SparseVector(1, {0: 1.0}), MSTATUS_one_hot=SparseVector(1, {}), numericalss=DenseVector([60.0, 14230.0]), numericals=DenseVector([60.0, 14230.0]), numericals_after_scale=DenseVector([6.9159, 1.7628]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing\n",
    "categoricals = [var for var in spark_df.columns if var.endswith(\"_one_hot\")]\n",
    "num = [\"numericals\"]\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols = categoricals + num, outputCol= \"features\")\n",
    "spark_df = vector_assembler.transform(spark_df)\n",
    "\n",
    "train_set, test_set = spark_df.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightCol is also available as an argument\n",
    "# regParam - regularization param that decreases ncreases the coefficients \n",
    "# the bigger the regParam the closer the coef to 0\n",
    "# this is done to discourage learning of more complex model and thus, overfitting\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "\n",
    "# FREQUENCY\n",
    "frequency =  GeneralizedLinearRegression(family=\"binomial\",\n",
    "                                           link=\"logit\",\n",
    "                                           #maxIter=10,\n",
    "                                           fitIntercept = True,\n",
    "                                           labelCol = \"CLAIM_FLAG\",\n",
    "                                           #regParam=0.3\n",
    "                                        )\n",
    "# customized name to the prediction col\n",
    "frequency.setPredictionCol(\"frq_pred\")\n",
    "\n",
    "# set the parameters \n",
    "para_grid = ParamGridBuilder()\\\n",
    "           .addGrid(frequency.regParam, [0.1, 0.3, 0.5, 0.7, 0.9])\\\n",
    "           .build()\n",
    "\n",
    "frq_evaluator = BinaryClassificationEvaluator(labelCol=\"CLAIM_FLAG\", rawPredictionCol='frq_pred', metricName='areaUnderROC')\n",
    "\n",
    "# GridSearchCV in sklearn\n",
    "frq_cross_val = CrossValidator(estimator = frequency,\n",
    "                           estimatorParamMaps= para_grid,\n",
    "                           evaluator = frq_evaluator,\n",
    "                              numFolds=3)\n",
    "\n",
    "fitted_frequency = frq_cross_val.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "frq_model = fitted_frequency.bestModel\n",
    "frq_model._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+\n",
      "|           frq_pred|CLAIM_FLAG|            features|\n",
      "+-------------------+----------+--------------------+\n",
      "| 0.4084206431036104|         0|(11,[0,1,3,7,9,10...|\n",
      "| 0.4546058774666069|         0|(11,[1,6,7,9,10],...|\n",
      "|  0.519013380964209|         1|[1.0,1.0,0.0,0.0,...|\n",
      "|0.49195439797650353|         1|[1.0,1.0,1.0,0.0,...|\n",
      "| 0.6366999586372941|         0|(11,[0,2,7,9,10],...|\n",
      "+-------------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "frq_predictions = frq_model.transform(test_set)\n",
    "\n",
    "# Select example rows to display.\n",
    "frq_predictions.select(\"frq_pred\", \"CLAIM_FLAG\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEVERITY\n",
    "train_set_severity = train_set.where((train_set[\"CLAIM_FLAG\"] > 0))\n",
    "\n",
    "severity =  GeneralizedLinearRegression(family=\"gamma\",\n",
    "                                           link=\"log\",\n",
    "                                           #maxIter=10,\n",
    "                                           fitIntercept = True,\n",
    "                                           labelCol = \"CLM_AMT\",\n",
    "                                           #regParam=0.3\n",
    "                                       )\n",
    "# customized name to the prediction col\n",
    "severity.setPredictionCol(\"sev_pred\")\n",
    "\n",
    "# cross validation \n",
    "para_grid = ParamGridBuilder()\\\n",
    "           .addGrid(frequency.regParam, [0.1, 0.3, 0.5, 0.7, 0.9])\\\n",
    "           .build()\n",
    "\n",
    "sev_evaluator = RegressionEvaluator(predictionCol=\"sev_pred\", labelCol=\"CLM_AMT\", metricName=\"mae\")\n",
    "\n",
    "sev_cross_val = CrossValidator(estimator = severity,\n",
    "                            estimatorParamMaps= para_grid,\n",
    "                            evaluator = sev_evaluator,\n",
    "                               numFolds=3)\n",
    "\n",
    "fitted_severity = sev_cross_val.fit(train_set_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best hyperparameters\n",
    "sev_model = fitted_severity.bestModel\n",
    "sev_model._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|          sev_pred|CLM_AMT|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|3860.4483393879086| 4386.0|[1.0,1.0,0.0,0.0,...|\n",
      "| 4388.005225467674| 3854.0|[1.0,1.0,1.0,0.0,...|\n",
      "| 5815.854536216155| 4400.0|(11,[0,1,2,7,9,10...|\n",
      "| 4877.560594427159| 2851.0|(11,[0,3,9,10],[1...|\n",
      "| 6680.578176346916| 2644.0|(11,[0,2,8,9,10],...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "MAE on test data = 3289.69\n"
     ]
    }
   ],
   "source": [
    "test_set_severity = test_set.where((test_set[\"CLAIM_FLAG\"] > 0))\n",
    "\n",
    "# Make predictions\n",
    "sev_predictions = sev_model.transform(test_set_severity)\n",
    "\n",
    "# Select example rows to display.\n",
    "sev_predictions.select(\"sev_pred\", \"CLM_AMT\", \"features\").show(5)\n",
    "\n",
    "mae = sev_evaluator.evaluate(sev_predictions)\n",
    "print(\"MAE on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the average of claims amount as a column\n",
    "mean_clm_amt = sev_predictions.agg({\"CLM_AMT\":\"avg\"}).take(1)[0][0]\n",
    "sev_predictions = sev_predictions.withColumn(\"CLM_AMT_MEAN\", F.lit(mean_clm_amt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-----------------+--------------------+\n",
      "|          sev_pred|CLM_AMT|     CLM_AMT_MEAN|            features|\n",
      "+------------------+-------+-----------------+--------------------+\n",
      "|3860.4483393879086| 4386.0|5287.752650176679|[1.0,1.0,0.0,0.0,...|\n",
      "| 4388.005225467674| 3854.0|5287.752650176679|[1.0,1.0,1.0,0.0,...|\n",
      "| 5815.854536216155| 4400.0|5287.752650176679|(11,[0,1,2,7,9,10...|\n",
      "| 4877.560594427159| 2851.0|5287.752650176679|(11,[0,3,9,10],[1...|\n",
      "| 6680.578176346916| 2644.0|5287.752650176679|(11,[0,2,8,9,10],...|\n",
      "+------------------+-------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sev_predictions.select(\"sev_pred\", \"CLM_AMT\",\"CLM_AMT_MEAN\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on test data id the model was to predcit the mean claim amount = 3081.09\n"
     ]
    }
   ],
   "source": [
    "sev_evaluator_mean = RegressionEvaluator(predictionCol=\"CLM_AMT_MEAN\", labelCol=\"CLM_AMT\", metricName=\"mae\")\n",
    "mae_mean = sev_evaluator_mean.evaluate(sev_predictions)\n",
    "print(\"MAE on test data id the model was to predcit the mean claim amount = %g\" % mae_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 3:\n",
    "Why Spark ML? Advantages and disadvantages <br>\n",
    "Integrated framework to perform advanced analytics\n",
    "\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-ec6f1cb336a9 <br> https://zaleslaw.medium.com/weakness-of-the-apache-spark-ml-library-41e674103591 <br> https://www.qubole.com/blog/apache-spark-use-cases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for the last few years the data has been growing exponentially and running a poerful ml model requires very powerful machines\n",
    "- high-end machines however is not advantageous (high costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "- simplicity and compatibility with tools coming from python and R\n",
    "- scalability - able to run the same code on a local machine and on a cluster as well - allows business to use the same workflow as the data grows\n",
    "- distributed computing speeds up the learning phase and creates better models\n",
    "- The MLlib can work in areas such as clustering, classification, and dimensionality reduction\n",
    "    - used in customer segmentation\n",
    "    - sentiment analysis\n",
    "    - to process streaming data\n",
    "- fast enough to perform exploratory queries without sampling\n",
    "    - complex data sets can be processed and visualised by combining spark with data visualization tools, \n",
    "\n",
    "#### Disadvanatges\n",
    "- a fewer algorithms in the SparkML lib\n",
    "- Spark ML has a very limited support of Pandas DataFrame functions - plotting, building advanced statistical functions, etc\n",
    "    - correlation between 2 columns can be calculated but it doesn't support ANOVA, different descriptive stats, hypothesis tests, etc\n",
    "- hard integration with TensorFlow and PyTorch\n",
    "- very basic neural network application - it has its own Keras-like API to build CNN,RNN, etc\n",
    "- spark is not designed as a multi-user environment\n",
    "    - spark users will need to know whether the memory they have access to will be sufficient for a dataset\n",
    "    - you need to coordinate memory usage with the team so you can run projects concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
